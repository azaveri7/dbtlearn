Notes:

The course overview can be found at:

https://github.com/dbtlearn/complete-dbt-bootcamp-zero-to-hero



The entire project has to be run in python virtual environment.

steps to do that:

* To install virtual env.
  brew install virtualenv

* To create virtual env.
  virtualenv venv

* To activate virtual env.
  . venv/bin/activate

any python package now that you install will get install in this venv.

Everytime you have to activate the venv while running our dbt dbt_project

* To read the executables again
  rehash

* To install snowflake plugin
  (venv)  pip install dbt-snowflake

* To make sure dbt executable is picked up
  (venv) rehash

* To identify dbt version
  which dbt

  /Users/anandzaveri/repos/courses/dbt-course/vent/bin/dbt


For e.g. if you exit venv and do dbt version

* which dbt
  /opt/homebrew/bin/dbt

In default version, we have not installed snowflake plugin. So our dbt_project
will not run with default dbt.

So to run our project, always activate venv thingy.

1. Incremental materialization

{{
    config(
        materialized = 'incremental',
        on_schema_change = 'fail'
    )
}}
WITH src_reviews AS (
    SELECT * FROM {{ ref('src_reviews') }}
)
SELECT * FROM src_reviews
WHERE review_text is not null
{% if is_incremental() %}
    AND review_date > (select max(review_date) from {{this}} )
{% endif %}

2. DBT command to load all incremental tables fully.

dbt run --full-refresh

This is useful when the schema of the source file / table is changed.

Using above command, all the incremental tables will be re-built.

3. ephemeral materialization

in this case, the models which are declared as ephemeral will be
available as CTE (common table expressions) to the dbt models
where they are referenced.

src:
      +materialized: ephemeral

With above lines, all the models in src folder will be ephemeral
and they will not be created in db schema.

However, if they were created before making them ephemeral, then
dbt will not automatically drop them. We need to drop them manually.

DROP VIEW AIRBNB.DEV.SRC_HOSTS;
DROP VIEW AIRBNB.DEV.SRC_LISTINGS;
DROP VIEW AIRBNB.DEV.SRC_REVIEWS;

4. To find the queries for materialized tables in form of CTE,
   in dbt_project.yml

   target-path: "target"  # directory which will store compiled SQL files

   so check target/run/dbtlearn/models/dim_listings_cleansed.sql, where we have referenced one of the views

   WITH src_listings AS (
    SELECT * FROM {{ ref('src_listings') }}
   )

   create or replace transient table airbnb.dev.dim_listings_cleansed  as
      (WITH  __dbt__cte__src_listings as (
WITH raw_listings AS (
    SELECT * FROM AIRBNB.RAW.RAW_LISTINGS
)

SELECT id AS listing_id,
    name AS listing_name,
    listing_url,
    room_type,
    minimum_nights,
    host_id,
    price AS price_str,
    created_at,
    updated_at
FROM
    raw_listings
),src_listings AS (
    SELECT * FROM __dbt__cte__src_listings
)

5. Few other snowflake queries:

select count(*) from AIRBNB.DEV.FCT_REVIEWS where LISTING_ID = 3176;

select count(*) from AIRBNB.RAW.RAW_REVIEWS where LISTING_ID = 3176;

insert into AIRBNB.RAW.RAW_REVIEWS values (3176, current_timestamp(), 'ANAND', 'Excellent stay', 'Positive');

6. Seeds used to copy csv files from dbt to db schema.

in dbt_project.yml, see below line:

seed-paths: ["seeds"]

curl https://dbtlearn.s3.us-east-2.amazonaws.com/seed_full_moon_dates.csv -o seeds/seed_full_moon_dates.csv

dbt seed

6. To check source freshness, add sources.yml in models folder to check fresh of all src models.

Replace this line

WITH raw_reviews AS (
    SELECT * FROM AIRBNB.RAW.RAW_REVIEWS
)

with

WITH raw_reviews AS (
    SELECT * FROM {{ source('airbnb','reviews')}}
)

and then create the sources.yml as follows:

version: 2

sources:
  - name: airbnb
    schema: raw
    tables:
      - name: listings
        identifier: raw_listings

      - name: hosts
        identifier: raw_hosts

      - name: reviews
        identifier: raw_reviews
        loaded_at_field: date
        freshness:
          warn_after: {count: 1, period: hour}
          error_after: {count: 24, period: hour}

Run the below command:

- dbt compile

and then

- dbt source freshness

In the output you can see,

01:38:32  1 of 1 START freshness of airbnb.reviews ....................................... [RUN]
01:38:35  1 of 1 ERROR STALE freshness of airbnb.reviews ................................. [ERROR STALE in 2.83s]

it shows error as our table has data whose date field is old more than 24 hours.

7. Snapshots

Snapshots live in snapshots folder. They implement Type 2 SCD ( slow changing dimensions ) tables.

2 strategies:

# Timestamp: a unique key and an updated_at field defined on the source model.
             These columns are used for determining changes.

# Check: Any change in set of columns ( or all columns ) will be picked up as
         as update.

scd_raw_listings.sql

{% snapshot scd_raw_listings %}

{{
    config(
        target_schema='dev',
        unique_key='id',
        strategy='timestamp',
        updated_at='updated_at',
        invalidate_hard_deletes=True
    )
}}

select * FROM {{ source('airbnb', 'listings') }}

{% endsnapshot %}

command:

dbt snapshot

In the snapshot table SCD_RAW_LISTINGS, 4 new columns are added viz.
DBT_SCD_ID
DBT_UPDATED_AT
DBT_VALID_FROM
DBT_VALID_TO

update AIRBNB.RAW.RAW_LISTINGS set MINIMUM_NIGHTS = 30, UPDATED_AT = CURRENT_TIMESTAMP()
WHERE ID = 3176

select * from AIRBNB.DEV.SCD_RAW_LISTINGS where id = 3176

In the SCD_RAW_LISTINGS, there is one single row.

DBT_VALID_FROM = 2009-06-05 21:34:42.000 DBT_VALID_TO = null

Then run

dbt snapshot

again checking,
select * from AIRBNB.DEV.SCD_RAW_LISTINGS where id = 3176

In the SCD_RAW_LISTINGS, there are 2 rows now.

MINIMUM_NIGHTS = 30, DBT_VALID_FROM = 2022-06-06 20:09:13.184 DBT_VALID_TO = null
MINIMUM_NIGHTS = 62, DBT_VALID_FROM = 2009-06-05 21:34:42.000 DBT_VALID_TO = 2022-06-06 20:09:13.184

The entire logic to implement snapshot is mostly written in connectors, in our case in dbt-snowflake connector.

8. Tests

2 types: singular and generic

# singular: SQL queries stored in tests which are expected to return an empty resultset.

# generic: 4 types of built-in generic tests.
  - unique
  - not_null
  - accepted_values
  - Relationships

Command to run tests

    - dbt test

Refer schema.yml for generic tests

version: 2

models:
  - name: dim_listings_cleansed
    columns:
      - name: listing_id
        tests:
          - unique
          - not_null

      - name: host_id
        tests:
          - not_null
          - relationships:
              to: ref('dim_hosts_cleansed')
              field: host_id

      - name: room_type
        tests:
          - accepted-values:
              values: ['Entire home/apt',
                       'Private room',
                       'Shared room',
                       'Hotel room']

The compiled queries can be found at target/compiled/dbtlearn/models/src/schema.yml

* Query for unique_dim_listings_cleansed_listing_id.sql

select
    listing_id as unique_field,
    count(*) as n_records

from airbnb.dev.dim_listings_cleansed
where listing_id is not null
group by listing_id
having count(*) > 1

* Query for not_null_dim_listings_cleansed_listing_id.sql

select listing_id
from airbnb.dev.dim_listings_cleansed
where listing_id is null

* Query for not_null_dim_listings_cleansed_host_id.sql

select host_id
from airbnb.dev.dim_listings_cleansed
where host_id is null

For singular tests, write a separate sql and store it in tests folder in dbt project.

Refer dim_listings_minimum_nights.sql

To run a selected test, u can use the command

dbt test --select dim_listings

dbt test --help

9. Macros

  They are the Jinja templates created in macros folder.

  Many built-in macros in dbt

  A special macro called test can be used for implementing generic tests.

  To install 3rd party packages, like dbt_utils in our project.
  create packages.yml at project level.

packages:
  - package: dbt-labs/dbt_utils
    version: 0.8.0

Then run

- dbt deps

It will install the packages mentioned in packages.yml

To use dbt_utils in fct_reviews.sql,

SELECT
{{ dbt_utils.surrogate_key(['listing_id','review_date','reviewer_name','review_text']) }} as review_id,
*
FROM
src_reviews

Creating a new column review_id in fct_reviews table which will combine all these cols and hash it.

Since fct_reviews is an incremental model and there is a schema change(adding a new col review_id),
we have to use --full-refresh option in dbt.

dbt run --full-refresh --select fct-reviews

10. Documentation

To generate documentation, add description tags in schema.yml
and then run command

dbt docs generate

Catalog written to /Users/anandzaveri/repos/courses/dbt-course/dbtlearn/target/catalog.json

cd target

ls -ltr

less catalog.json

json is not user friendly, so to convert to html doc, lets serve this doc via document server.
Any other web server can also be used.

dbt docs serve

http://127.0.0.1:8080/#!/overview

* To include custom documentation into docs.
- in schema.yml, add below line
  description: '{{ doc("dim_listing_cleansed__minimum_nights") }}'

- create docs.md with below content in models folder

{% docs dim_listing_cleansed__minimum_nights %} Minimum number of nights required to rent this property.

Keep in mind that old listings might have minimum_nights set to 0 in the source tables. Our cleansing algorithm updates this to 1.

{% enddocs %}

* To view the DAG, click on the Green button on docs screen at http://127.0.0.1:8080/#!/overview

* if you select any table in Databases in the doc server and then click on DAG button, data flow
for that table will be shown in DAG.



* To include any image in the documentation.

- create a folder assets and copy the image over there
- create overview.md
- in the dbt_project.yml, add below line:
    asset-paths: ["assets"]
- and give reference of this in overview.md

Here is the schema of our input data:
![input schema](assets/input_schema.png)

11. Analyses, Hooks and Exposures

Analyses query reside in analyses folder in dbt project.

then do dbt compile

The analyses query will be located at target/compiled/dbtlearn/analyses

The query generated in this file can be ran against our data warehouse.

less target/compiled/dbtlearn/analyses/full_moon_no_sleep.sql

Hooks

Hooks are SQLs that are executed during predefined times.

Can be configured on project, model or subfolder level.

on_run_start: executed at the start of dbt
on_run_end: executed at the end of dbt
pre-hook: executed before a model/seed/snapshot is built
post-hook: executed after a model/seed/snapshot is built

models:
  dbtlearn:
    +materialized: view
    +post-hook:
      - "GRANT SELECT ON {{ this }} TO ROLE REPORTER"

So this post-hook will be executed for every dbt model in dbtlearn package.
{{ this }} will be replace by actual model at runtime.

Prest - A BI tool to be integrated with DBT and snowflake

Create a workspace in Preset and chart and dashboard and copy the URL

https://152517ae.us2a.app.preset.io/superset/dashboard/p/7m8rgKwg1zY/

Exposures can be used to connect BI tool to DBT.

Refer dashboards.yml for the same.

The use

dbt docs generate

dbt docs serve

In the link http://localhost:8080

there will be a new section
    Exposures
        Dashboard

    Click on it and finally you can see the DBTlearn Dashboard

View the lineage graph and you can see the DBTLearn Dashboard added in the lineage graph.