Notes:

The course overview can be found at:

https://github.com/dbtlearn/complete-dbt-bootcamp-zero-to-hero



The entire project has to be run in python virtual environment.

steps to do that:

* To install virtual env.
  brew install virtualenv

* To create virtual env.
  virtualenv venv

* To activate virtual env.
  . venv/bin/activate

any python package now that you install will get install in this venv.

Everytime you have to activate the venv while running our dbt dbt_project

* To read the executables again
  rehash

* To install snowflake plugin
  (venv)  pip install dbt-snowflake

* To make sure dbt executable is picked up
  (venv) rehash

* To identify dbt version
  which dbt

  /Users/anandzaveri/repos/courses/dbt-course/vent/bin/dbt


For e.g. if you exit venv and do dbt version

* which dbt
  /opt/homebrew/bin/dbt

In default version, we have not installed snowflake plugin. So our dbt_project
will not run with default dbt.

So to run our project, always activate venv thingy.

1. Incremental materialization

{{
    config(
        materialized = 'incremental',
        on_schema_change = 'fail'
    )
}}
WITH src_reviews AS (
    SELECT * FROM {{ ref('src_reviews') }}
)
SELECT * FROM src_reviews
WHERE review_text is not null
{% if is_incremental() %}
    AND review_date > (select max(review_date) from {{this}} )
{% endif %}

2. DBT command to load all incremental tables fully.

dbt run --full-refresh

This is useful when the schema of the source file / table is changed.

Using above command, all the incremental tables will be re-built.

3. ephemeral materialization

in this case, the models which are declared as ephemeral will be
available as CTE (common table expressions) to the dbt models
where they are referenced.

src:
      +materialized: ephemeral

With above lines, all the models in src folder will be ephemeral
and they will not be created in db schema.

However, if they were created before making them ephemeral, then
dbt will not automatically drop them. We need to drop them manually.

DROP VIEW AIRBNB.DEV.SRC_HOSTS;
DROP VIEW AIRBNB.DEV.SRC_LISTINGS;
DROP VIEW AIRBNB.DEV.SRC_REVIEWS;

4. To find the queries for materialized tables in form of CTE,
   in dbt_project.yml

   target-path: "target"  # directory which will store compiled SQL files

   so check target/run/dbtlearn/models/dim_listings_cleansed.sql, where we have referenced one of the views

   WITH src_listings AS (
    SELECT * FROM {{ ref('src_listings') }}
   )

   create or replace transient table airbnb.dev.dim_listings_cleansed  as
      (WITH  __dbt__cte__src_listings as (
WITH raw_listings AS (
    SELECT * FROM AIRBNB.RAW.RAW_LISTINGS
)

SELECT id AS listing_id,
    name AS listing_name,
    listing_url,
    room_type,
    minimum_nights,
    host_id,
    price AS price_str,
    created_at,
    updated_at
FROM
    raw_listings
),src_listings AS (
    SELECT * FROM __dbt__cte__src_listings
)

5. Few other snowflake queries:

select count(*) from AIRBNB.DEV.FCT_REVIEWS where LISTING_ID = 3176;

select count(*) from AIRBNB.RAW.RAW_REVIEWS where LISTING_ID = 3176;

insert into AIRBNB.RAW.RAW_REVIEWS values (3176, current_timestamp(), 'ANAND', 'Excellent stay', 'Positive');

6. Seeds used to copy csv files from dbt to db schema.

in dbt_project.yml, see below line:

seed-paths: ["seeds"]

curl https://dbtlearn.s3.us-east-2.amazonaws.com/seed_full_moon_dates.csv -o seeds/seed_full_moon_dates.csv

dbt seed

6. To check source freshness, add sources.yml in models folder to check fresh of all src models.

Replace this line

WITH raw_reviews AS (
    SELECT * FROM AIRBNB.RAW.RAW_REVIEWS
)

with

WITH raw_reviews AS (
    SELECT * FROM {{ source('airbnb','reviews')}}
)

and then create the sources.yml as follows:

version: 2

sources:
  - name: airbnb
    schema: raw
    tables:
      - name: listings
        identifier: raw_listings

      - name: hosts
        identifier: raw_hosts

      - name: reviews
        identifier: raw_reviews
        loaded_at_field: date
        freshness:
          warn_after: {count: 1, period: hour}
          error_after: {count: 24, period: hour}

Run the below command:

- dbt compile

and then

- dbt source freshness

In the output you can see,

01:38:32  1 of 1 START freshness of airbnb.reviews ....................................... [RUN]
01:38:35  1 of 1 ERROR STALE freshness of airbnb.reviews ................................. [ERROR STALE in 2.83s]

it shows error as our table has data whose date field is old more than 24 hours.

7. Snapshots

Snapshots live in snapshots folder. They implement Type 2 SCD ( slow changing dimensions ) tables.

2 strategies:

# Timestamp: a unique key and an updated_at field defined on the source model.
             These columns are used for determining changes.

# Check: Any change in set of columns ( or all columns ) will be picked up as
         as update.

scd_raw_listings.sql

{% snapshot scd_raw_listings %}

{{
    config(
        target_schema='dev',
        unique_key='id',
        strategy='timestamp',
        updated_at='updated_at',
        invalidate_hard_deletes=True
    )
}}

select * FROM {{ source('airbnb', 'listings') }}

{% endsnapshot %}

command:

dbt snapshot

In the snapshot table SCD_RAW_LISTINGS, 4 new columns are added viz.
DBT_SCD_ID
DBT_UPDATED_AT
DBT_VALID_FROM
DBT_VALID_TO

update AIRBNB.RAW.RAW_LISTINGS set MINIMUM_NIGHTS = 30, UPDATED_AT = CURRENT_TIMESTAMP()
WHERE ID = 3176

select * from AIRBNB.DEV.SCD_RAW_LISTINGS where id = 3176

In the SCD_RAW_LISTINGS, there is one single row.

DBT_VALID_FROM = 2009-06-05 21:34:42.000 DBT_VALID_TO = null

Then run

dbt snapshot

again checking,
select * from AIRBNB.DEV.SCD_RAW_LISTINGS where id = 3176

In the SCD_RAW_LISTINGS, there are 2 rows now.

MINIMUM_NIGHTS = 30, DBT_VALID_FROM = 2022-06-06 20:09:13.184 DBT_VALID_TO = null
MINIMUM_NIGHTS = 62, DBT_VALID_FROM = 2009-06-05 21:34:42.000 DBT_VALID_TO = 2022-06-06 20:09:13.184

The entire logic to implement snapshot is mostly written in connectors, in our case in dbt-snowflake connector.

8. Tests

2 types: singular and generic

# singular: SQL queries stored in tests which are expected to return an empty resultset.

# generic: 4 types of built-in generic tests.
  - unique
  - not_null
  - accepted_values
  - Relationships

Command to run tests

    - dbt test

Refer schema.yml for generic tests

version: 2

models:
  - name: dim_listings_cleansed
    columns:
      - name: listing_id
        tests:
          - unique
          - not_null

      - name: host_id
        tests:
          - not_null
          - relationships:
              to: ref('dim_hosts_cleansed')
              field: host_id

      - name: room_type
        tests:
          - accepted-values:
              values: ['Entire home/apt',
                       'Private room',
                       'Shared room',
                       'Hotel room']

The compiled queries can be found at target/compiled/dbtlearn/models/src/schema.yml

* Query for unique_dim_listings_cleansed_listing_id.sql

select
    listing_id as unique_field,
    count(*) as n_records

from airbnb.dev.dim_listings_cleansed
where listing_id is not null
group by listing_id
having count(*) > 1

* Query for not_null_dim_listings_cleansed_listing_id.sql

select listing_id
from airbnb.dev.dim_listings_cleansed
where listing_id is null

* Query for not_null_dim_listings_cleansed_host_id.sql

select host_id
from airbnb.dev.dim_listings_cleansed
where host_id is null

For singular tests, write a separate sql and store it in tests folder in dbt project.

Refer dim_listings_minimum_nights.sql

To run a selected test, u can use the command

dbt test --select dim_listings

dbt test --help

9. Macros

  They are the Jinja templates created in macros folder.

  Many built-in macros in dbt

  A special macro called test can be used for implementing generic tests.
